import streamlit as st
import pandas as pd
import json
import uuid
from backend_codes import build_workflow
from langchain_core.messages import HumanMessage, AIMessage

# --- 1. ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®æº–å‚™ï¼ˆåˆå›å®Ÿè¡Œæ™‚ã®ã¿ï¼‰ ---
# @st.cache_resourceã‚’ä½¿ã†ã“ã¨ã§ã€ã‚¢ãƒ—ãƒªã®å†å®Ÿè¡Œæ™‚ã«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å†æ§‹ç¯‰ã™ã‚‹ã®ã‚’é˜²ãã€é«˜é€ŸåŒ–ã—ã¾ã™ã€‚
@st.cache_resource
def get_workflow():
    return build_workflow()

compiled_workflow = get_workflow()

# --- 2. Streamlitãƒšãƒ¼ã‚¸ã®åŸºæœ¬è¨­å®š ---
st.title("ğŸ¤– ãƒ‡ãƒ¼ã‚¿åˆ†æãƒãƒ£ãƒƒãƒˆAI")
st.caption("ã“ã®AIã¯ã€è£å´ã§LangGraphã¨ã„ã†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")

# -ã‚»ãƒƒã‚·ãƒ§ãƒ³ã”ã¨ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’åˆæœŸåŒ– ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []
# ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’è‡ªå‰ã§ç®¡ç†ã™ã‚‹å ´åˆ
if "session_id" not in st.session_state:
    st.session_state["session_id"] = str(uuid.uuid4())

# --- 4. éå»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã™ã¹ã¦è¡¨ç¤º ---
# st.session_state.messagesã«ã¯ã€{"role": "user", "content": ...} ã‚„ {"role": "assistant", "content": ...} ã®å½¢å¼ã§ä¿å­˜
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # contentãŒæ–‡å­—åˆ—ã®å ´åˆï¼ˆé€šå¸¸ã®ä¼šè©±ï¼‰
        if isinstance(message["content"], str):
            st.markdown(message["content"])
        # contentãŒè¾æ›¸ã®å ´åˆï¼ˆãƒ‡ãƒ¼ã‚¿ã‚„ã‚°ãƒ©ãƒ•ã‚’å«ã‚€ï¼‰
        elif isinstance(message["content"], dict):
            if "result_df_json" in message["content"] and message["content"]["result_df_json"]:
                df_data = json.loads(message["content"]["result_df_json"])
                st.dataframe(pd.DataFrame(df_data))
            if "fig_json" in message["content"] and message["content"]["fig_json"]:
                fig = pd.io.json.read_json(message["content"]["fig_json"], typ='frame')
                st.plotly_chart(fig)
            if "interpretation" in message["content"] and message["content"]["interpretation"]:
                 st.markdown(message["content"]["interpretation"])


# --- 5. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ã‚’å—ã‘ä»˜ã‘ ---
user_input = st.chat_input("åˆ†æã—ãŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹: ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å£²ä¸Šã‚’è¦‹ã›ã¦ï¼‰")

if user_input:
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’å±¥æ­´ã«è¿½åŠ ã—ã¦è¡¨ç¤º
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # AIã®å¿œç­”ã‚’å¾…ã¤é–“ã€ã‚¹ãƒ”ãƒŠãƒ¼ï¼ˆãã‚‹ãã‚‹å›ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ï¼‰ã‚’è¡¨ç¤º
    # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚¹ãƒšãƒ¼ã‚¹ã‚’ç¢ºä¿
    with st.chat_message("assistant"):
        
        # st.spinnerã®ä»£ã‚ã‚Šã«st.statusã‚’ä½¿ç”¨ã€‚å®Ÿè¡ŒçŠ¶æ³ã‚’å‹•çš„ã«æ›´æ–°ã§ãã‚‹
        with st.status("AIãŒãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åˆ†æä¸­...", expanded=True) as status:
            try:
                # LangGraphãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®å‘¼ã³å‡ºã—è¨­å®š
                config = {"configurable": {"thread_id": st.session_state.session_id}}
                input_data = {"messages": [HumanMessage(content=user_input)]}
                
                # .invoke()ã®ä»£ã‚ã‚Šã«.stream()ã‚’ä½¿ã„ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
                for chunk in compiled_workflow.stream(input_data, config):
                    
                    # --- chunkã‚’è§£æã—ã¦ã€å®Ÿè¡ŒçŠ¶æ³ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤º ---
                    if "supervisor" in chunk:
                        # ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒã‚¤ã‚¶ãƒ¼ãŒæ€è€ƒä¸­
                        status.update(label="ã©ã®å°‚é–€å®¶ã«ä¾é ¼ã™ã‚‹ã‹æ€è€ƒä¸­...")
                    elif "sql_node" in chunk:
                        # SQLãƒãƒ¼ãƒ‰ãŒå®Ÿè¡Œä¸­
                        status.update(label="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€SQLã‚’å®Ÿè¡Œä¸­...")
                    elif "processing_node" in chunk:
                        # ãƒ‡ãƒ¼ã‚¿åŠ å·¥/ã‚°ãƒ©ãƒ•ä½œæˆãƒãƒ¼ãƒ‰ãŒå®Ÿè¡Œä¸­
                        status.update(label="ãƒ‡ãƒ¼ã‚¿ã‚’åŠ å·¥ã—ã€ã‚°ãƒ©ãƒ•ã‚’ä½œæˆä¸­...")
                    elif "interpret_node" in chunk:
                        # è§£é‡ˆãƒãƒ¼ãƒ‰ãŒå®Ÿè¡Œä¸­
                        status.update(label="çµæœã‚’è§£é‡ˆã—ã€èª¬æ˜ã‚’ç”Ÿæˆä¸­...")
                    
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®æœ€å¾Œã«ã¯ã€æœ€çµ‚çŠ¶æ…‹ãŒ'__end__'ã‚­ãƒ¼ã§å«ã¾ã‚Œã‚‹
                    if "__end__" in chunk:
                        final_state = chunk["__end__"]

                    status.update(label="åˆ†æå®Œäº†ï¼", state="complete", expanded=False)
                    response_messages = final_state["messages"]
                    ai_response_to_display = None

                    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æœ«å°¾ã‹ã‚‰é¡ã£ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è¡¨ç¤ºã™ã¹ãã€Œæœ¬å½“ã®å¿œç­”ã€ã‚’æ¢ã™
                    for i in range(len(response_messages) - 1, -1, -1):
                        msg = response_messages[i]
                        
                        # ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒã‚¤ã‚¶ãƒ¼ã®æœ€çµ‚åˆ¤æ–­ï¼ˆDispatchDecisionï¼‰ã«é–¢é€£ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ã‚¹ã‚­ãƒƒãƒ—
                        if isinstance(msg, AIMessage) and msg.tool_calls:
                            if msg.tool_calls[0]['name'] == 'DispatchDecision':
                                continue
                        if isinstance(msg, HumanMessage):
                            # äººé–“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¾ã§é¡ã£ãŸã‚‰æ¢ç´¢çµ‚äº†
                            break

                        # ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œãªã‹ã£ãŸæœ€åˆã®AIã¾ãŸã¯Toolãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã€è¡¨ç¤ºã™ã¹ãå¿œç­”
                        ai_response = msg
                        break
                    
                    # ã‚‚ã—è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã€å¿µã®ãŸã‚æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½¿ã†
                    if ai_response_to_display is None:
                        ai_response_to_display = response_messages[-1]

                    
                    response_content = None
                    
                    # è¿”ç­”ã®å½¢å¼ã«å¿œã˜ã¦å†…å®¹ã‚’è§£æ
                    if isinstance(ai_response, AIMessage):
                        # é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”
                        response_content = {"interpretation": ai_response.content}

                    elif hasattr(ai_response, 'content') and isinstance(ai_response.content, str):
                        try:
                            # JSONå½¢å¼ã®æ–‡å­—åˆ—ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚„ã‚°ãƒ©ãƒ•ï¼‰
                            parsed_content = json.loads(ai_response.content)
                            
                            # ToolMessageã‹ã‚‰è¿”ã£ã¦ãã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ã•ã‚‰ã«ãƒã‚¹ãƒˆã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€ä¸­èº«ã‚’å–ã‚Šå‡ºã™
                            df_json = parsed_content.get("result_df_json")
                            fig_json = parsed_content.get("fig_json")
                            
                            response_content = {
                                "result_df_json": df_json,
                                "fig_json": fig_json
                            }

                        except json.JSONDecodeError:
                            # JSONã§ã¯ãªã„ãŸã ã®ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”
                            response_content = {"interpretation": ai_response.content}

                    # è§£æã—ãŸAIã®å¿œç­”ã‚’å±¥æ­´ã«è¿½åŠ 
                    st.session_state.messages.append({"role": "assistant", "content": response_content})
                    
                    # ç”»é¢ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ã€æœ€æ–°ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤º
                    st.rerun()

            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
